# CUA GPU Configuration for Ubuntu 24.04 with Ampere GPUs
# Copy this file to .env.local and customize for your setup

# API Keys (required for cloud models)
ANTHROPIC_API_KEY=your_anthropic_key_here
OPENAI_API_KEY=your_openai_key_here
GOOGLE_API_KEY=your_google_api_key_here

# GPU Configuration
CUDA_VISIBLE_DEVICES=0  # Comma-separated list of GPU IDs to use (e.g., 0,1,2)
CUDA_DEVICE_ORDER=PCI_BUS_ID  # Use PCI bus order for consistent device ordering

# PyTorch Optimization for Ampere GPUs
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True  # Better memory management
TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0  # Ampere compute capabilities
CUDA_LAUNCH_BLOCKING=0  # Enable async kernel launches for better performance
TORCH_CUDNN_V8_API_ENABLED=1  # Enable cuDNN v8 API

# Memory Management
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512  # Reduce memory fragmentation

# Inference Optimization
TORCH_BACKENDS_CUDA_MATMUL_ALLOW_TF32=1  # Enable TF32 for faster matmul (Ampere+)
TORCH_BACKENDS_CUDNN_ALLOW_TF32=1  # Enable TF32 for cuDNN (Ampere+)
TORCH_BACKENDS_CUDNN_BENCHMARK=1  # Auto-tune cuDNN kernels

# Model Loading
TRANSFORMERS_CACHE=~/.cache/huggingface  # HuggingFace model cache directory
TORCH_HOME=~/.cache/torch  # PyTorch model cache directory

# Local Model Configuration (for GPU-accelerated local inference)
# Uncomment and set for local models
# HF_MODEL_PATH=/path/to/local/model
# DEVICE=cuda  # or cuda:0, cuda:1, etc.

# Docker Configuration
DOCKER_GPU_RUNTIME=nvidia  # NVIDIA Docker runtime
DOCKER_GPU_CAPABILITIES=compute,utility  # Required capabilities

# Logging and Monitoring
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR
CUDA_LAUNCH_BLOCKING=0  # Set to 1 for debugging CUDA errors (slower)

# Performance Tuning
OMP_NUM_THREADS=8  # Number of CPU threads for parallel operations
MKL_NUM_THREADS=8  # Intel MKL threads

# Multi-GPU Configuration (if using multiple GPUs)
# NCCL_DEBUG=INFO  # Enable NCCL debugging
# NCCL_P2P_DISABLE=0  # Enable P2P communication between GPUs
# NCCL_IB_DISABLE=1  # Disable InfiniBand (set to 0 if you have IB)

# CUA Cloud Configuration (optional)
# CUA_API_KEY=your_cua_cloud_api_key
# CUA_CLOUD_ENDPOINT=https://api.cua.ai

# Container Configuration
CONTAINER_SHARED_MEMORY=2g  # Shared memory for Docker containers
CONTAINER_MEMORY_LIMIT=16g  # Memory limit for containers
CONTAINER_CPU_LIMIT=4  # CPU limit for containers
